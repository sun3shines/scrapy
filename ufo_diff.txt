urllib2.urlopen

搜集知乎上所有和爬虫相关的技术了。是的。然后总和，和总结了。是的。


使用代理，，爬，现在。

    <span style="font-size:18px;">import urllib2  
    proxy_support = urllib2.ProxyHandler({'http':'http://XX.XX.XX.XX:XXXX'})  
    opener = urllib2.build_opener(proxy_support, urllib2.HTTPHandler)  
    urllib2.install_opener(opener)  
    content = urllib2.urlopen('http://XXXX').read()</span>  


    <span style="font-size:18px;">import urllib, urllister  
      
    def getURL(url):  
        try:  
            usock = urllib.urlopen(url)  
        except:  
            print 'get url excepton'  
            return []  
        parser = urllister.URLLister()  
        parser.feed(usock.read())  
        usock.close()  
        parser.close()  
        urls = parser.urls  
        return urls</span>  


    <span style="font-size:18px;">import urllib2  
    content = urllib2.urlopen('http://XXXX').read()</span>  


还是需要培养我们自己的想法。小的想法从一点一点的开始了。等其发挥了变化了，进行了溢出后，再谋其他了。所以就从最简单，最小的功能开始了。

培养我们的主旋律。

如何选择最小，也是一个问题啊。不能一上来就特别大了。否则被扯淡了。就。是的。

进阶发展一。从零开始进阶列表。

抓取第一个页面。
先搞好代理，和安全工作。至于内容后续考虑。比如说header，浏览器的header等。

先搞好防爬虫机制。这个搞的好了后，在搞批量了。是的。所以说，先一个个页面的页面的搞了。是的。在一个一次抓取的页面中，把所有的内容都搞好了。比如说cookie，和header，和代理了。是的。确实是比较费脑子的。不想别人那样想写就写了。

然后搞代理了。 都在一次任务中完成了。 包括x-forworade等的关键字了。


然后多个代理同时抓取了。是的。


目标网站CSDN

尤其要先做好安全工作，被封号了，就扯淡了。是的。

学习各种的库。是的。

研究下几个urllib2 等的库，然后研究下反反爬虫的机制。

关于urllib2中的几个对象的理解？opener？和builder?

读书读书，文章内部内容的分析。

研究知乎上的爬虫技术了。

知乎上的爬虫学习，不是其有多巅峰，而是其能为我们产生效益了。是的。关键是要看代码的风格了。是的。

接触新鲜事物，接触和消化的过程中，接触和消化并且实践，就是牛逼了。所以，还是需要非常的投入的。是的。

首先就是要做好安全工作了。是的。是的。等到世界的变化了。最重要了。

还是要自己写一个HTTP服务器。自己解析了？

持续的投入，引起了事情的变化和溢出了。但是阅读，不会引起变化了。只有投入，和实践，会引起事情的变化了。是的。

所以啊，就是干了。晚上回去，做点事情，引起事情的变化了。而不是我们阅读，只能引起我们心情的变化了。是的。

所以啊，还是要等着事情的溢出了。搜寻所有的事情的了。是的。

因为有年轻，所以要把时间多多的投入了。是的。

github上写帐号，然后多个同时工作了。是的。不同分支来搞了。是的。

通过自己的干，来让事情发生变化了。是的。发挥学习的能力。

爬虫中，我们接触的都是新的内容。但是不要有畏惧。是的。有投入就有变化，有变化，就有进步了。是的。

难的是不能长久的投入了。

build_opener默认添加几个处理器，但提供快捷的方法来添加或更新默认处理器。 什么鬼？以及parent ？？

学习的时候，不能直接上知乎。因为没有教程，需要一个教程，先从小到大的去养了。

我们不学习，是因为我们要对其进行深入研究了？而不是利用其的结果了？
貌似是啊。

create database scrapy;

create table link( id bigint NOT NULL PRIMARY KEY AUTO_INCREMENT,url text NOT NULL,time datetime,state int NOT NULL);

时间，获取后还是需要重新更新的。

代码continue太多，可读性，不好如何解决？

现在的关键是如何构建一个模型了。一个加速的模型了。决策模型了。是的。
目前的策略，就是如果没有达到关键node的限制，就快速的达到了。是的。而不是瓜分数据库了。是的。如何瓜分数据库的数据？这个不需要考虑了。是的。就是直接的快速瓜分了。是的。

为何没有精确的限制呢？why？

# 当等于2的数量超过100时，数据库，不再添加ids，则发送wait了。是的。发送命令，让其崩溃，如何？

现在客户端进行限制，后期进行通信，获取从服务端来的各种配置了。是的。增加url配置了。即，controller负责保存配置。并把配置发送给客户端。然后客户端自己进行判断，进行限制了

每个微小的功能，都有可能掀起一场革命了。

增加python文件锁 ？ 防止克隆拷贝的时候，被滥用了？

进程推出后，会进一步修改文件锁了。

读文件不靠谱，写数字的时候。打开读。最好写的时候，可以关闭。或者重新写。即使feek依然不可已了。

在controller中发送相关的信息给node，然后node自己来执行了。发送任务的解析片段。和所解析的内容，或者信息了。是的。

然后此处的内容最好还是可以去重构一下了。 这个礼拜，最好是能够把分布式爬虫给写完了。进程控制已经搞定了。是的。

代码，能否按照层次进行优化？

如何把continue去掉？采用yield 方式？

以及很多的if和else，continue等。

限制bfs和dfs，也就限制了total了。
