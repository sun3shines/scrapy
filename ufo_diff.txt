urllib2.urlopen

搜集知乎上所有和爬虫相关的技术了。是的。然后总和，和总结了。是的。


使用代理，，爬，现在。

    <span style="font-size:18px;">import urllib2  
    proxy_support = urllib2.ProxyHandler({'http':'http://XX.XX.XX.XX:XXXX'})  
    opener = urllib2.build_opener(proxy_support, urllib2.HTTPHandler)  
    urllib2.install_opener(opener)  
    content = urllib2.urlopen('http://XXXX').read()</span>  


    <span style="font-size:18px;">import urllib, urllister  
      
    def getURL(url):  
        try:  
            usock = urllib.urlopen(url)  
        except:  
            print 'get url excepton'  
            return []  
        parser = urllister.URLLister()  
        parser.feed(usock.read())  
        usock.close()  
        parser.close()  
        urls = parser.urls  
        return urls</span>  


    <span style="font-size:18px;">import urllib2  
    content = urllib2.urlopen('http://XXXX').read()</span>  


还是需要培养我们自己的想法。小的想法从一点一点的开始了。等其发挥了变化了，进行了溢出后，再谋其他了。所以就从最简单，最小的功能开始了。

培养我们的主旋律。

如何选择最小，也是一个问题啊。不能一上来就特别大了。否则被扯淡了。就。是的。

进阶发展一。从零开始进阶列表。

抓取第一个页面。
先搞好代理，和安全工作。至于内容后续考虑。比如说header，浏览器的header等。

先搞好防爬虫机制。这个搞的好了后，在搞批量了。是的。所以说，先一个个页面的页面的搞了。是的。在一个一次抓取的页面中，把所有的内容都搞好了。比如说cookie，和header，和代理了。是的。确实是比较费脑子的。不想别人那样想写就写了。

然后搞代理了。 都在一次任务中完成了。 包括x-forworade等的关键字了。


然后多个代理同时抓取了。是的。


目标网站CSDN

尤其要先做好安全工作，被封号了，就扯淡了。是的。

学习各种的库。是的。

研究下几个urllib2 等的库，然后研究下反反爬虫的机制。

关于urllib2中的几个对象的理解？opener？和builder?

读书读书，文章内部内容的分析。

研究知乎上的爬虫技术了。

知乎上的爬虫学习，不是其有多巅峰，而是其能为我们产生效益了。是的。关键是要看代码的风格了。是的。

接触新鲜事物，接触和消化的过程中，接触和消化并且实践，就是牛逼了。所以，还是需要非常的投入的。是的。

首先就是要做好安全工作了。是的。是的。等到世界的变化了。最重要了。

还是要自己写一个HTTP服务器。自己解析了？

持续的投入，引起了事情的变化和溢出了。但是阅读，不会引起变化了。只有投入，和实践，会引起事情的变化了。是的。

所以啊，就是干了。晚上回去，做点事情，引起事情的变化了。而不是我们阅读，只能引起我们心情的变化了。是的。

所以啊，还是要等着事情的溢出了。搜寻所有的事情的了。是的。

因为有年轻，所以要把时间多多的投入了。是的。

github上写帐号，然后多个同时工作了。是的。不同分支来搞了。是的。

通过自己的干，来让事情发生变化了。是的。发挥学习的能力。

爬虫中，我们接触的都是新的内容。但是不要有畏惧。是的。有投入就有变化，有变化，就有进步了。是的。

难的是不能长久的投入了。

build_opener默认添加几个处理器，但提供快捷的方法来添加或更新默认处理器。 什么鬼？以及parent ？？

学习的时候，不能直接上知乎。因为没有教程，需要一个教程，先从小到大的去养了。

我们不学习，是因为我们要对其进行深入研究了？而不是利用其的结果了？
貌似是啊。

create database scrapy;

create table link( id bigint NOT NULL PRIMARY KEY AUTO_INCREMENT,url text NOT NULL,time datetime,state int NOT NULL);

时间，获取后还是需要重新更新的。

代码continue太多，可读性，不好如何解决？

现在的关键是如何构建一个模型了。一个加速的模型了。决策模型了。是的。
目前的策略，就是如果没有达到关键node的限制，就快速的达到了。是的。而不是瓜分数据库了。是的。如何瓜分数据库的数据？这个不需要考虑了。是的。就是直接的快速瓜分了。是的。

为何没有精确的限制呢？why？

# 当等于2的数量超过100时，数据库，不再添加ids，则发送wait了。是的。发送命令，让其崩溃，如何？

现在客户端进行限制，后期进行通信，获取从服务端来的各种配置了。是的。增加url配置了。即，controller负责保存配置。并把配置发送给客户端。然后客户端自己进行判断，进行限制了

每个微小的功能，都有可能掀起一场革命了。

增加python文件锁 ？ 防止克隆拷贝的时候，被滥用了？

进程推出后，会进一步修改文件锁了。

读文件不靠谱，写数字的时候。打开读。最好写的时候，可以关闭。或者重新写。即使feek依然不可已了。

在controller中发送相关的信息给node，然后node自己来执行了。发送任务的解析片段。和所解析的内容，或者信息了。是的。

然后此处的内容最好还是可以去重构一下了。 这个礼拜，最好是能够把分布式爬虫给写完了。进程控制已经搞定了。是的。

代码，能否按照层次进行优化？

如何把continue去掉？采用yield 方式？

以及很多的if和else，continue等。

限制bfs和dfs，也就限制了total了。

可以在达到进程的限制后，继续创建线程了。可以这样搞。对于已达到限制的进程，比如说根进程，开始产生线程了。是的。

去掉available ，增加trytimes 是的。作用和avaliable类似了。

关于。position解析的设置。
获取到url。获取到页面。开线程解析position。在线程关闭的时候解析是合适的。是的。
或者是对于url进行开线程解析了。是的。获取到不同的页面了。把获取到的页面和url以键值的方式放入到队列中么？
解析页面，没有IO，所以不需要进行线程了。是的。然后获取到已获取到的page了。此时的url，用于设置state2了。
对于linkposition，则url2对应于结果url0了。对于非linkposition，则url2对应与[]
了。而一个页面中，即有linkposition，也有非linkposition了。而非linkposition，则用来存储了。比如说poxyip了。

对于页面，依次使用position，进行解析了。linkposition怎么办？那就返回

因此，那么我们现在就是开始对于页面，进行编了。进行规划了。是的。

好好设计，把层次搞清楚，后面的就可以随便的浪了。是的。

关于自由的写，和利用别人的写。有什么区别么？

那么对于proxyposition，该如何处理呢？解析后，如何可用，则直接使用了。是的。并且，会自动使用position代理了。是的。

所以那么现在的思路，就是一旦启动后，代理就会启动了。同时启动代理的解析了。是的。即任何的一个

一个网站，或者url，对于其的position的set了。是的。

而且解析出来的代理，url会被同时的发送使用了。

那么我们平时只要搜集代理的ip，就可以了。是的。

同时解析一遍，因为不清楚，那些的ip是否可用了。而且网站上应该会有更新了。是的。所以，会找速度快，的代理ip了。是的。会先启动代理吧。等可活跃的代理数量足够多的时候，再开启我们的主任务预热了。是的。到后期，两个任务会进行融合了。是的。

一在这个上面，那么我的思路是很牛逼的。但是为什么在现实中，和人相处时，思路就傻逼了。就停滞了？擦。

每个网站，会有自己的一套规则了。是的。

而对于代理ip，则其的用法也是不同的了。是的。代理ip还是存到数据库较为合适了。是的。读或者写了，因为所有的进程都要可以进行访问了。所以，是的。

所以特定position.特定存储了。比如说对于代理IP，那就和url一样，也是存储在表中了。如果没有，那就只能用自己的了。是的。

以及开其他的线程，验证代理ip的有效性了。是的。

慢慢布局，做好代理ip这块。和url一样了。是的。

代理ip这块的功能，需要好好的合计下。判断下其的基本流程了。是的。

代理ip的基本流程了。
实时的使用了，每个十分钟，会清除本进程的所有的代理了。是的。

不浮躁，慢慢布局了。慢慢研究了。

关于那个什么。关于代理ip的使用方式。住去url时，采用代理ip了。会随机配置相应的代理ip了？是的。那么。url，附加相应的代理ip了。是的。减少node节点的复杂度。从controller中获取等等。是的。

一开始，会默认产生几个了。是的。访问不同的代理ip。存储到数据库中了。并且在controller中，开启新的线程，进行验证了。

然后proxypositioin和linkposition就完成了统一处理了。是的。

此时就是无差别的处理了。牛逼啊。是的。判断代理ip，是否为本机ip了。如果为本机ip，则不开线程。主线程执行了。是的。如果是代理ip，则允许执行了。是的。

做一个完成的代理管理了。现在多干点，作为我们的积累了。是的。

先存一些ip吧，还是。

搜集ip的总量。对于每个端口进行常识了。如果代理ip端口，可用，则可用。不可用，则扯淡了。是的。

这个就是代理ip的管理了。是的。

创建IP 表。表明属性。和主机 ？or代理？。是的。 活动？or不活动。

每个IP或者主机，对应着一套的解析规则了。是的。

即，此处，应该是代理IP管理了。增加代理website ，代理的集合了。是的。

id,proxyhost

id,ip,port,type,active 0，为检测，1，不可用，2，可用。一上来，会默认设置了。并使用线程，进行连接了。是的。

不包括本机ip，如果查不到适合的ip，则使用本机ip了。 是的。

代理ip的流程先跑通。

爬代理ip的网站，和爬其他网站的方式其实是一样的。 是的。

先写一个积累的方式了。

node对于解析出来的ip不验证，直接，入库。因为不符合设计理念了符合解析要求的数据。至于可用要求，这个是针对于存储后的专门处理了。是的。

create table proxyip( id bigint NOT NULL PRIMARY KEY AUTO_INCREMENT,ip varchar(32) NOT NULL,port int NOT NULL,active int NOT NULL);

create table proxysite( id bigint NOT NULL PRIMARY KEY AUTO_INCREMENT,host varchar(255) NOT NULL,active int NOT NULL);

编写数据库接口，插入原始数据。编写后台线程了。 在插入的时候，会进行验证端口的可用了。是的。 要求速度快的。

后台验证线程了。

对于website的操作。只有插入了。是的。选择好host，然后插入了。是的。

proxyip，insert（批量），update（批量），select（批量limit）

proxysite，insert 

关于数据库的随机处理？如何？搞之？

SELECT * FROM link WHERE id >= ((SELECT MAX(id) FROM link)-(SELECT MIN(id) FROM link)) * RAND() + (SELECT MIN(id) FROM link)  LIMIT 100

SELECT * FROM link WHERE id >= ((SELECT MAX(id) FROM link)-(SELECT MIN(id) FROM link)) * RAND() + (SELECT MIN(id) FROM link)  AND state=1 LIMIT 1000;

研究下后台进程的proxy的业务了。 以及作为批量初始化的情况了。

需要有去重操作了。是的。

对于scoket资源采用锁的方式访问了。那为什么之前的线程没事呢？统计下开了多少的锁了。是的。

随机从数据库队列中抓取么？抓取limit数量的ip。如果不够，就一直循环的去抓了。是的 .最好是可以从队列中循环抓取了。是的

所以我们的lock是不会在mysql对象上添加的。而是让操作之间去互相去抢锁了。而有些操作是不需要锁的。比如说读取active的ip了。是的。

为url配置proxyip了。是的。

# 任务分发，任务处理，任务存储。

# 对于结果需要收集的。进行一个任务转发。比如说linkpostion，和proxypostion了。是的。

# 以id和url作为每个position中的唯一标志符了。是的。

# 对于新的结果处理方式，我看还是要分开的好。只是返回需要处理的。id了，表示此id已完成了。但是可以慢慢发了。

# 因此，还是putget一体了。以及，我们的任务处理从我们的进程管理中脱离出来了。是的。

# putget 不涉及到结果的回首和处理了。而结果的回首处理由xx来进行了。是的。
# 这个很重要。把link作为结果，从put/get中解脱处理，这样就简单多了。是的。单纯的读和修改了。

# 以及link的插入操作，会被作为merget来处理了。是的以及proxy的merget来处理了。的。

# 所以，现在需要开一个线程了。增加对于link和proxy的merget了。是的。分离主义。分离主义，要更加的模块化了。是的。

# 但是，进程会等待所有的任务都完成了。是的。所以说进程和线程还是有管理了。因为进程必须等待线程完成后，自己才能去做其他的事情了。是的。

# 增加datamerget
# 的各种后台线程了。是的。除了link，和proxy是线程。其他的merget，可以作为进程来处理了。是的。

# 因此，我们可以这样理解。即link的增加，不归dispatch所管。而是由新的模块来负责。其只负责减少了。是的。负责消耗未完成的ids了。完成，继续请求，继续消耗了。是的。

测试网页阶段。不使用多进程了。是的。

多线程处理时，不等待，完成多少，就发送多少结果。如果导致了超时，那么所有的任务都会被重置的。是的。

如果这样，那么有可能是put 和get
还是要分离可？先不处理这个了。这个作为后续的调整了。目前，先把 关键的内容处理好了。 一般来说，不会出现这个任务。因为访问的速度都是可以的，所以，代理的内容也是可以的。中间不会有太大的差距了。

proxyip 表中active 状态说明

0 初始化
1 可用
2 不可用
3 被检测状态

link中state的状态说明
0 新插入
1 已取出，正在检测 。每次访问数据库时若超时则会被置为0
2 已完成

sleep时间太长了。是的。

对于返回的结果attrs 进行过滤。后续使用了。

可以开线程，而且等待线程结束。因为线程和url的抓取处理解析并不冲突。可以等待所有的线程结束后，去进行fork了。是的。

线程超时怎么办？或者失败怎么办？那就不返回了。后续让数据来处理了。更新数据库的处理了？

代理管理已解决。现在是要解决我们的线程处理。处理线程。和返回值的问题了。是的。比如说解析出来的url了？因为现在put
get是一致性的。所以要求，线程结束后。其的结果，需要进行保存了。是的。

还是说线程结束后，不是直接返回给进程。而是由进程来统一收集了？也是对于存储一致性的管理了。linkposition。和urlposition？的处理了？以及proxyposition了？是的。

因此，任务接受。任务分发，任务解析。任务存储。结果回收。是的。

不能允许线程过于自由。比如说put和get的分开。那么我们将无法对于进程进行拷贝了。是的。不好。

修改进行测试。而不是修改代码了。或者说注释代码了。是的。

感觉有问题啊。那些进程处理的？

因为reset的问题，导致了现有的内容发生了变化。重复执行了。速率没有提升。所以目前是准备把reset的工作，放到后，交由专门的线程来维护了。访问这些超时的内容时，肯定是没有会访问的吧。不冲突。应该。

这个涉及到了link的管理了。现在我们先不处理了。后续的处理了。一般不会过分的超时了。是的。

作为后，会写成包，然后回家安装，抓图片了。是的。

proxyip表增加type http？orhttps varchar（8）

为什么大公司不封匿名端口？

增加 scheme anonymous

alter table proxyip add column scheme varchar(8);

alter table proxyip add column anonymous varchar(8); 

alter table proxyip add column abroad varchar(8);

目前先过滤掉https吧。以及，先如何搞之？

国外的不好使，还是国内的好。

制作成为rpm包，以及mysql数据库搭建 了。

mysqldump -u root -p111111 scrapy > dbscrapy.bak

如mysql -u root -p

mysql>use 数据库名

然后使用source命令，后面参数为脚本文件(如这里用到的.sql)

mysql>source /home/bak.sql

tar -jcvf scrapy-1.1.tar.gz scrapy --exclude=.svn

制作common，server，scrapy等的rpm包。要求标准化。是的。

self.root
需要变更了。不为其parent，是的。而是后面三个。判断是否为祖先，则是写path == root了。

关于爬虫的进一步改进。即，一个客户端，myscrapy.py
处理一个site了。而所有需要的postion，都在此myscrapy中处理了。以及该myscrapy进程，只处理和该site相关的url了。即，发送的url了？是的。因此，客户端，发送请求时，有自己的标志符了。而插入到数据库，url带有此标志符。且返回该标志的url了。是的。

然后proxydaemon，就可以剥离了。每个网站，都有自己的scrapy客户端。等于是注册了。以及，写任务了。写position了。是的。写出来链接，即可了。是的。可以进行配置了。且全局配置，是在此客户端中进行写配置了。而不是本地配置了。是的。

由主机，和客户端，来设定本机的值了。或者说是全局配置了。是的。

现在是进程共享/scrapy 了。所以需要区分出来st_dir 和 root_dir 了。

        # 此时self.http.status 结果可以保存，留在合适的 机会进行判断了
        # p.run 和 p.new 可以统一执行了，根据self.http.status
        # 来进行判断了。是的。以及其的状态。
        # 这个就是面向对象的好处了。是的。为什么呢？因为判断的问题么？
        # DRY 原则。
        # 因为之前是因为条件只能判断一次，所以函数就会执行多次，现在为了函数执行一次，那么条件判断
        # 就执行多次了

面向结构，都是一次判断的。以情况判断的。但是在面向对象中则不是，可以多次判断了。是的。所以，看着好看，但是可读性不好了。

状态会被记忆的问题呢？状态会被保留了？

需要有一个状态clear的过程了。是的。

#  input,output ，此时为controller的大使了。不是钥匙。不是对立的。而是一致的。 顾问大使，钥匙和锁，枪和弹

create table scrapy( id bigint NOT NULL PRIMARY KEY AUTO_INCREMENT,uuid varchar(255) NOT NULL,starturl varchar(255) NOT NULL);

create table link( id bigint NOT NULL PRIMARY KEY AUTO_INCREMENT,url text NOT NULL,time datetime,state int NOT NULL,uid bigint NOT NULL);

alter table link add constraint uuid_check foreign key(uid) references scrapy(id) on delete cascade on update cascade;

把代理，当成普通的处理了。是的

conf中，host和parent不能通过conf获取了。因为进程fork的问题。是的。进程fork的问题，导致了配置不同。但是，在运行中能否修改呢？算了。

完成pynginx，ufs，以及scrapy的收尾工作。

URLS_LIMIT 由客户端来处理。

保存url，作为page作为文件来处理，如何？应该可以吧。哈哈。

下一步，proxy模块的独立处理了。现在就告一段落吧。即每个url，都会有自己的proxy服务了。而不是从controller中获取了。即，再次，从controller中进行分离了。是的。分离服务了。

理解为media资源。和link资源了。是的。或者proxy资源了。基本上就是这三种资源了。是的。然后下载呢，就是分开了。是的。使用独立的内容来下载了。是的。给media创建类型了？是的。如果是html，jpg，txt等类型。是的。 对应网站，csdn。是的。

因此其所解析出来的内容，都是media了。是的。

create table media(id bigint NOT NULL PRIMARY KEY AUTO_INCREMENT,url text NOT NULL,time timestamp,state int NOT NULL,uid bigint NOT NULL);
alter table media add constraint media_check foreign key(uid) references scrapy(id) on delete cascade on update cascade;

find . -name '*.py'  | awk -F ':' '{print $1}' | xargs -i sed -i 's/scrapy.center/fastscrapy.center/g' {}

find . -name '*.py'  | awk -F ':' '{print $1}' | xargs -i sed -i 's/scrapy.common/fastscrapy.common/g' {}

find . -name '*.py'  | awk -F ':' '{print $1}' | xargs -i sed -i 's/scrapy.controller/fastscrapy.controller/g' {}

find . -name '*.py'  | awk -F ':' '{print $1}' | xargs -i sed -i 's/scrapy.globalx/fastscrapy.globalx/g' {}

find . -name '*.py'  | awk -F ':' '{print $1}' | xargs -i sed -i 's/scrapy.parse/fastscrapy.parse/g' {}

find . -name '*.py'  | awk -F ':' '{print $1}' | xargs -i sed -i 's/scrapy.storage/fastscrapy.storage/g' {}

find . -name '*.py'  | awk -F ':' '{print $1}' | xargs -i sed -i 's/scrapy.test/fastscrapy.test/g' {}

find . -name '*.py'  | awk -F ':' '{print $1}' | xargs -i sed -i 's/scrapy.utils/fastscrapy.utils/g' {}

find . -name '*.py'  | awk -F ':' '{print $1}' | xargs -i sed -i 's/scrapy.worker/fastscrapy.worker/g' {}

find . -name '*.py'  | awk -F ':' '{print $1}' | xargs -i sed -i 's/cloudcommon/cloudlib/g' {}
